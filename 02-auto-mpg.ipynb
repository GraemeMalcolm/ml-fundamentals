{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Predicting Automobile Fuel Efficiency\r\n",
    "\r\n",
    "## A simple regression example\r\n",
    "\r\n",
    "In this notebook, we'll explore some techniques for training  *regression* machine learning model that predicts the miles-per-gallon fuel efficiency of an automobile based on its features.\r\n",
    "\r\n",
    "### Explore and clean the data\r\n",
    "\r\n",
    "Let's start by loading the automobile data into a Pandas dataframe and looking at the first 20 instances."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\r\n",
    "\r\n",
    "# load the training dataset\r\n",
    "auto_data = pd.read_csv('data/auto-mpg.csv')\r\n",
    "auto_data.head(20)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The data we have to work with includes *features* of automobiles, such as the model, number of cylinders, engine displacement, horsepower, weight, accelleration, model year, and origin (North America, Europe, or Asia), and also the *label* that we want to train a model to predict (*mpg*).\r\n",
    "\r\n",
    "Let's see how many observations we have in our dataset."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "len(auto_data)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can also count the number of values in each column:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "auto_data.count()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Hmm, it looks like there are some missing values in the *horsepower* column. We confirm the number of null values for each column by using the **isnull** function, like this:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "auto_data.isnull().sum()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's see the records with missing values in context:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "auto_data[auto_data.isnull().any(axis=1)]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The *NaN* value indicates that the value is \"not a number\" - in this case because it is missing.\r\n",
    "\r\n",
    "There are a few ways we can deal with missing values like this. For example, we could substitute the missing values with a reasonable value, such as the mean horsepower for all of the other records. However, sometimes the easiest thing to do (assuming you have enough remaining data with which to train a model) is to simply eliminate the rows with missing data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "auto_data = auto_data.dropna(axis=0, how='any')\r\n",
    "auto_data.count()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Alright, now that we've cleaned up the data, let's see what we have. First, let's identify the data types in our dataset:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "auto_data.dtypes"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Some of the data variables are numeric (in this case, 64-bit integers and floating-point values), while others are objects (often text values, or *strings*).\r\n",
    "\r\n",
    "Let's look at the statistical description of the dataframe values (this applies only to numeric columns):"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "auto_data.describe()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The *mpg* column contains the *label* value that we want to train a model to predict. The other columns contain the numeric *features* that might help predict the label.\r\n",
    "\r\n",
    "In general, there are two kinds of numeric feature:\r\n",
    "\r\n",
    "- *Continuous* features - values that represent numeric values you would typically *measure*, and which could be fractional\r\n",
    "- *Discrete* features - values that represent discrete quantities that you would typically *count*, and which are typically whole numbers.\r\n",
    "\r\n",
    "While discrete numeric values often represent quantities, they are sometimes used as *categorical* variables - in other words, rather than representing a quantity, they can be viewed as category indicators. In the case of our automobile data, the *model_year* variable is an obvious example of this - the value 1970, does not represent a quantity of 1,970.00; but rather the year 1970.\r\n",
    "\r\n",
    "A less obvious example might be *cylinders*. Even though this represents a quantity, the spread of the values is from 3 to 8; so while this value does indeed represent the number of cylinders a car has, you could also view it as a category that groups cars into 3-cylinder cars, 4- cylinder cars, 8-cylinder cars, and so on). In a real machine learning project, you'd likely do some more analysis of the data to determine how best to treat this variable, but for the purposes of this exercise, we'll consider it as categorical rather than a numeric value.\r\n",
    "\r\n",
    "So, let's explicitly define the *features* we want to consider as numeric values, along with the numeric *label* we want to train a model to predict (*mpg*):"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "numeric_features = ['displacement', 'horsepower', 'weight', 'accelleration']\r\n",
    "auto_data[numeric_features + ['mpg']].describe()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's now focus on the label that our model will try to predict: *mpg*.\r\n",
    "\r\n",
    "Let's use matplotlib to plot a histogram and a box plot so we can understand the distribution of mpg values in the sample data we have."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\r\n",
    "\r\n",
    "# This ensures plots are displayed inline in the Jupyter notebook\r\n",
    "%matplotlib inline\r\n",
    "\r\n",
    "# Get the label column\r\n",
    "label = auto_data['mpg']\r\n",
    "\r\n",
    "\r\n",
    "# Create a figure for 2 subplots (2 rows, 1 column)\r\n",
    "fig, ax = plt.subplots(2, 1, figsize = (9,12))\r\n",
    "\r\n",
    "# Plot the histogram   \r\n",
    "ax[0].hist(label, bins=100)\r\n",
    "ax[0].set_ylabel('Frequency')\r\n",
    "\r\n",
    "# Add lines for the mean, median, and mode\r\n",
    "ax[0].axvline(label.mean(), color='magenta', linestyle='dashed', linewidth=2)\r\n",
    "ax[0].axvline(label.median(), color='cyan', linestyle='dashed', linewidth=2)\r\n",
    "\r\n",
    "# Plot the boxplot   \r\n",
    "ax[1].boxplot(label, vert=False)\r\n",
    "ax[1].set_xlabel(label.name)\r\n",
    "\r\n",
    "# Add a title to the Figure\r\n",
    "fig.suptitle('Distribution')\r\n",
    "\r\n",
    "# Show the figure\r\n",
    "fig.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The distribution of *mpg* values is slightly *left skewed*; in other words, the bulk of the data is at the lower end of the scale, with fewer instances of cars with extremely high values. However, the mean and median values are not too far from the center of the distribution; so while the data is not spread across what statisticians call a *normal* distribution, it seems reasonably balanced. There are no apparent *outliers* that would represent unusually high or low values.\r\n",
    "\r\n",
    "Now let's examine the distributions of the various numeric features."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Plot a histogram for each numeric feature\r\n",
    "for col in numeric_features:\r\n",
    "    fig = plt.figure(figsize=(9, 6))\r\n",
    "    ax = fig.gca()\r\n",
    "    feature = auto_data[col]\r\n",
    "    feature.hist(bins=100, ax = ax)\r\n",
    "    ax.axvline(feature.mean(), color='magenta', linestyle='dashed', linewidth=2)\r\n",
    "    ax.axvline(feature.median(), color='cyan', linestyle='dashed', linewidth=2)\r\n",
    "    ax.set_title(col)\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Again, there don't appear to be any significant outliers. The distribution of *accelleration* looks more or less *normal* (with low values at the extremes and most of the data peaking in the middle at the mean value), while the others tend to be *left-skewed*.\r\n",
    "\r\n",
    "Now let's explore the relationships bwteen the numeric features and the label. To do this we'll create *scatterplot* for each variable that plots the intersection of each feature and label value. We'll also calculate a statistics called *correlation*, which measures the strength of the relationship between numeric variables on scale between -1 and + 1. A correlation of +1 indicates that high values of one variable tend to coincide with high values of the other; a correlation of -1 indicates that high values of one variable tend to coincide with low values of the other, and a correlation of 0 indicates that there is no discernible relationship between the variables."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Plot a scatterpolot for each numeric feature vs the label\r\n",
    "for col in numeric_features:\r\n",
    "    fig = plt.figure(figsize=(9, 6))\r\n",
    "    ax = fig.gca()\r\n",
    "    y = auto_data[col]\r\n",
    "    x = auto_data['mpg']\r\n",
    "    plt.scatter(x, y)\r\n",
    "    cor = auto_data['mpg'].corr(auto_data[col])\r\n",
    "    ax.set_title(col + ' vs mpg (correlation: ' + str(round(cor, 2)) + ')')\r\n",
    "    ax.set_ylabel(col)\r\n",
    "    ax.set_xlabel('mpg')\r\n",
    "plt.show()\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The scatterplots for *displacement*, *horsepower*, and *weight*  show a distinct diagonal pattern in which cars with high values for these features tend to have lower *mpg* values. This visual pattern is supported by the negative correlation scores calculated for these columns. The *acceleration* feature however has a slightly positive correlation with the label, in which cars with higher accelleration also tend to have higher *mpg* values.\r\n",
    "\r\n",
    "The key takeaway from the perspective of building a predictive model, is that there do seem to be relationships between all of these features and the label we're trying to predict - so all of these features may be useful in trying to predict the unknown *mpg* of a car.\r\n",
    "\r\n",
    "### Train a predictive model\r\n",
    "\r\n",
    "OK, now it's time for our first attempt at training a model that can predict the *mpg* of a car based on some of its features. We'll use the numeric features we've been exploring, and which appear to have a relationship with the *mpg*.\r\n",
    "\r\n",
    "Predicting numeric values, such as *mpg*, is a form of machine learning known as *regression*. Regression is an example of a *supervised* machine learning technique, in which a dataset containing known label values is used to train a model, which can then be applied to new data for which the label is unknown. The first step, is to identify the *features* we want to use to train the model, and the *label* we want to train it to predict. The following code separates these into two arrays of dfata, and prints the first 10 elements of each.\r\n",
    "\r\n",
    "> **Note**: By convention, we often refer to the feature values as **X** and the label values as **y** - in effect, training a machine learning model is the process of determining a function (**f**) that operates on **X** to calculate **y**, or mathematically, ***f*(X)=y**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Separate features and labels\r\n",
    "X, y = auto_data[numeric_features].values, label.values\r\n",
    "print('Features:',X[:10], '\\nLabels:', y[:10], sep='\\n')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, because we have a dataset containing known label values, we can use some rows to train a model, but hold some rows back that later we can use to test the model and see how close the predicted labels for the hold-back data are to the actual label values that we already know. To accomplsih this, we'll use the Scikit-Learn library's *train_test_split* function to randomly split the data into a training set (which consists of an array of features and the corresponding array of labels) and a test set (which again, consists of an array of features and a corresponding array of labels).\r\n",
    "\r\n",
    "It's common to use most of the data to train the model, and a smaller subset to test it - a 70%/30% split is a typical starting point."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "# Split data 70%-30% into training set and test set\r\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)\r\n",
    "\r\n",
    "print ('Training Set: %d rows\\nTest Set: %d rows' % (X_train.shape[0], X_test.shape[0]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "With the data split into train and test subsets, we can now go ahead and use the training data to train a model. To do this, we need to select the training *algorithm* that we want to use to *fit* the features and labels in the training data to a predictive model. There are many different kinds of algorithm for regression, and in this case, we'll start with one of the simplest and most well-established of these - *linear regression*, which attempts to establish a *linear* relationship between the feature values and the label values."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Train the model\r\n",
    "from sklearn.linear_model import LinearRegression\r\n",
    "\r\n",
    "# Fit a linear regression model on the training set\r\n",
    "model = LinearRegression().fit(X_train, y_train)\r\n",
    "print (model.coef_)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The *coefficients* for the trained model are displayed - these are the values that the linear regression algorithm has determined should be applied to the feature values in order to calculate the predicted label value.\r\n",
    "\r\n",
    "Let's see what labels the model predicts for the test dataset we held back previously, and compare those predicted label values to the actual label values we know to be true:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\r\n",
    "\r\n",
    "predictions = model.predict(X_test)\r\n",
    "np.set_printoptions(suppress=True)\r\n",
    "print('Predicted labels: ', np.round(predictions, 1)[:10])\r\n",
    "print('Actual labels   : ' ,y_test[:10])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "So how did it do?\r\n",
    "\r\n",
    "We can compare each predicted value to the actual value, and get a sense that the model is reasonably close for each prediction; but that's quite laborious, and makes it hard to get an overall view of how well the model predicts.\r\n",
    "\r\n",
    "An alternative approach might be to visualize the predicted and the actual values as a scatter plot, so we can see how well they line up."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.scatter(y_test, predictions)\r\n",
    "plt.xlabel('Actual Labels')\r\n",
    "plt.ylabel('Predicted Labels')\r\n",
    "plt.title('Predicted vs Actual Labels')\r\n",
    "# overlay the regression line\r\n",
    "z = np.polyfit(y_test, predictions, 1)\r\n",
    "p = np.poly1d(z)\r\n",
    "plt.plot(y_test,p(y_test), color='magenta')\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The scatterplot shows a diagonal trend in which the predicted values align more or less linearly with the actual values - we've added a *trend line* to the chart to make it more obvious. It's not perfect, but definitely seems to have some predictive capability.\r\n",
    "\r\n",
    "What we might want to do is to try a few other algorithms to see if we can produce a better model, and while we could visually compare scatterplots for each model's predictions, it's easier to evaluate model performance if we can quantify the predictive performance of each model as a simple metric that can be compared. Fotunately, there are many such metrics we can calculate, including:\r\n",
    "\r\n",
    "- **Mean Square Error (MSE)**: The mean of the squared differences between predicted and actual values. This is a *relative* metric in which the smaller the value, the better the fit of the model\r\n",
    "- **Root Mean Square Error (RMSE)**: The square root of the MSE. This is an *absolute* metric in the same unit as the label (in this case, *mpg*). Again, the smaller the value, the better the model.\r\n",
    "- **Coefficient of Determination (usually known as *R-squared* or R<sup>2</sup>)**: A *relative* metric between 0 and 1 in which the higher the value, the better the fit of the model. Essentially, R<sup>2</sup> represents how much of the variance between predicted and actual label values the model is able to explain.\r\n",
    "\r\n",
    "Let's see these metrics for our linear regression model."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\r\n",
    "\r\n",
    "mse = mean_squared_error(y_test, predictions)\r\n",
    "print(\"MSE:\", mse)\r\n",
    "\r\n",
    "rmse = np.sqrt(mse)\r\n",
    "print(\"RMSE:\", rmse)\r\n",
    "\r\n",
    "r2 = r2_score(y_test, predictions)\r\n",
    "print(\"R2:\", r2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "OK, now that we have quantified the predictive performance of our model, let's try a different kind of algorithm and see if it gives us better results. This time, we'll use a *tree-based* model, in which the features in the dataset are examined in a series of evaluations, each of which results in a *branch* in a *decision tree* based on the feature value. At the end of each series of branches are leaf-nodes with the predicted label value based on the feature values.\r\n",
    "\r\n",
    "To see this in action, we'll train a *Decision Tree* regression model using the automobile data. After training the model, the code below will print the model definition and a text representation of the tree it uses to predict label values."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Train the model\r\n",
    "from sklearn.tree import DecisionTreeRegressor\r\n",
    "from sklearn.tree import export_text\r\n",
    "\r\n",
    "# Fit a linear regression model on the training set\r\n",
    "model = DecisionTreeRegressor().fit(X_train, y_train)\r\n",
    "print (model)\r\n",
    "\r\n",
    "# Visualize the model tree\r\n",
    "tree = export_text(model)\r\n",
    "print(tree)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "So, how does our tree-based model perform with our test data?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "predictions = model.predict(X_test)\r\n",
    "\r\n",
    "plt.scatter(y_test, predictions)\r\n",
    "plt.xlabel('Actual Labels')\r\n",
    "plt.ylabel('Predicted Labels')\r\n",
    "plt.title('Predictions')\r\n",
    "# overlay the regression line\r\n",
    "z = np.polyfit(y_test, predictions, 1)\r\n",
    "p = np.poly1d(z)\r\n",
    "plt.plot(y_test,p(y_test), color='magenta')\r\n",
    "plt.show()\r\n",
    "\r\n",
    "mse = mean_squared_error(y_test, predictions)\r\n",
    "print(\"MSE:\", mse)\r\n",
    "\r\n",
    "rmse = np.sqrt(mse)\r\n",
    "print(\"RMSE:\", rmse)\r\n",
    "\r\n",
    "r2 = r2_score(y_test, predictions)\r\n",
    "print(\"R2:\", r2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Well, it looks like the decision tree model performs slightly worse than the linear regression model. Given the linear relationships we observed previously between the numeric features and the label, it's perhaps not all that suprising that a linear model works well.\r\n",
    "\r\n",
    "However, let;s not be hasty - there are other algorithms we can try, including *ensemble* methods that actually combine multiple base algorithms to produce an optimal model, either by applying an aggregate function to a collection of base models (sometimes referred to a *bagging*) or by building a sequence of models that build on one another to improve predictive performance (referred to as *boosting*).\r\n",
    "\r\n",
    "We'll try a *gradient boosting* ensemble model that tries to combine a sequence of models that minimize the *loss* (error) in predictions by determining the curve (*gradient*) for a function that calculates the loss, and adjusting the model coefficients so that the loss value is reduced (a technique known generally as *gradient descent*)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Train the model\r\n",
    "from sklearn.ensemble import GradientBoostingRegressor\r\n",
    "\r\n",
    "# Fit a linear regression model on the training set\r\n",
    "model = GradientBoostingRegressor().fit(X_train, y_train)\r\n",
    "print (model)\r\n",
    "\r\n",
    "predictions = model.predict(X_test)\r\n",
    "\r\n",
    "plt.scatter(y_test, predictions)\r\n",
    "plt.xlabel('Actual Labels')\r\n",
    "plt.ylabel('Predicted Labels')\r\n",
    "plt.title('Predictions')\r\n",
    "# overlay the regression line\r\n",
    "z = np.polyfit(y_test, predictions, 1)\r\n",
    "p = np.poly1d(z)\r\n",
    "plt.plot(y_test,p(y_test), color='magenta')\r\n",
    "plt.show()\r\n",
    "\r\n",
    "mse = mean_squared_error(y_test, predictions)\r\n",
    "print(\"MSE:\", mse)\r\n",
    "\r\n",
    "rmse = np.sqrt(mse)\r\n",
    "print(\"RMSE:\", rmse)\r\n",
    "\r\n",
    "r2 = r2_score(y_test, predictions)\r\n",
    "print(\"R2:\", r2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "That's a little better than the original linear regression algorithm, we're making progress!\r\n",
    "\r\n",
    "Now, to improve even further, we can try adjusting some parameters in the algorithm to control how it behaves. Technically, parameters in machine learning algorithms are called *hyper*parameters (you can think of the feature values themselves as the parameters for the function produced by the training process, so we use the term hyperparameters to be clear that we mean parameters that are set outside of the training data).\r\n",
    "\r\n",
    "in the case of the gradient boosting algorithm, there are several hyperparameters we can experiment with; including the *learning rate* (the size of the adjustments made to coefficients to optimize the loss function) and *n_estimators* (the number of estimators combined to for the ensemble sequence). Let's try setting these values explicitly."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Train the model\r\n",
    "from sklearn.ensemble import GradientBoostingRegressor\r\n",
    "\r\n",
    "# Fit a linear regression model on the training set\r\n",
    "model = GradientBoostingRegressor(learning_rate=0.5, n_estimators=50).fit(X_train, y_train)\r\n",
    "print (model)\r\n",
    "\r\n",
    "predictions = model.predict(X_test)\r\n",
    "\r\n",
    "plt.scatter(y_test, predictions)\r\n",
    "plt.xlabel('Actual Labels')\r\n",
    "plt.ylabel('Predicted Labels')\r\n",
    "plt.title('Predictions')\r\n",
    "# overlay the regression line\r\n",
    "z = np.polyfit(y_test, predictions, 1)\r\n",
    "p = np.poly1d(z)\r\n",
    "plt.plot(y_test,p(y_test), color='magenta')\r\n",
    "plt.show()\r\n",
    "\r\n",
    "mse = mean_squared_error(y_test, predictions)\r\n",
    "print(\"MSE:\", mse)\r\n",
    "\r\n",
    "rmse = np.sqrt(mse)\r\n",
    "print(\"RMSE:\", rmse)\r\n",
    "\r\n",
    "r2 = r2_score(y_test, predictions)\r\n",
    "print(\"R2:\", r2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We get a slightly different result from before, so clearly the hyperparameters affect the model.\r\n",
    "\r\n",
    "But how can we chose the best hyperparameter values? Well, we can use a technique knows as *hyperparameter sweeping*, which tries multiple combinations of parameter values until we find the ones that produce the best model (based on the metric we specify).\r\n",
    "\r\n",
    "let's try hyperparameter sweeping to find the model with the best r<sup>2</sup> metric."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import GridSearchCV\r\n",
    "from sklearn.metrics import make_scorer, r2_score\r\n",
    "\r\n",
    "# Use a Gradient Boosting algorithm\r\n",
    "alg = GradientBoostingRegressor()\r\n",
    "\r\n",
    "# Try these hyperparameter values\r\n",
    "params = {\r\n",
    " 'learning_rate': [0.1, 0.5, 1.0],\r\n",
    " 'n_estimators' : [50, 100, 150]\r\n",
    " }\r\n",
    "\r\n",
    "# Find the best hyperparameter combination to optimize the R2 metric\r\n",
    "score = make_scorer(r2_score)\r\n",
    "gridsearch = GridSearchCV(alg, params, scoring=score, cv=3, return_train_score=True)\r\n",
    "gridsearch.fit(X_train, y_train)\r\n",
    "print(\"Best parameter combination:\", gridsearch.best_params_, \"\\n\")\r\n",
    "\r\n",
    "# Get the best model\r\n",
    "model=gridsearch.best_estimator_\r\n",
    "print(model, \"\\n\")\r\n",
    "\r\n",
    "# Evaluate the model using the test data\r\n",
    "predictions = model.predict(X_test)\r\n",
    "mse = mean_squared_error(y_test, predictions)\r\n",
    "print(\"MSE:\", mse)\r\n",
    "rmse = np.sqrt(mse)\r\n",
    "print(\"RMSE:\", rmse)\r\n",
    "r2 = r2_score(y_test, predictions)\r\n",
    "print(\"R2:\", r2)\r\n",
    "\r\n",
    "# Plot predicted vs actual\r\n",
    "plt.scatter(y_test, predictions)\r\n",
    "plt.xlabel('Actual Labels')\r\n",
    "plt.ylabel('Predicted Labels')\r\n",
    "plt.title('Predictions')\r\n",
    "# overlay the regression line\r\n",
    "z = np.polyfit(y_test, predictions, 1)\r\n",
    "p = np.poly1d(z)\r\n",
    "plt.plot(y_test,p(y_test), color='magenta')\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Pretty good - we've now improved the model a little more.\r\n",
    "\r\n",
    "### Use categorical features\r\n",
    "\r\n",
    "Up to now, we've trained the model using only the numeric features in the dataset. However, there are some categorical features that we could also make use of. Let's take a look at those."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\r\n",
    "\r\n",
    "# plot a bar plot for each categorical feature count\r\n",
    "categorical_features = ['car_name','cylinders','model_year','origin']\r\n",
    "\r\n",
    "for col in categorical_features:\r\n",
    "    counts = auto_data[col].value_counts().sort_index()\r\n",
    "    fig = plt.figure(figsize=(9, 6))\r\n",
    "    ax = fig.gca()\r\n",
    "    counts.plot.bar(ax = ax, color='steelblue')\r\n",
    "    ax.set_title(col + ' counts')\r\n",
    "    ax.set_xlabel(col) \r\n",
    "    ax.set_ylabel(\"Frequency\")\r\n",
    "plt.show()\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "There are (not unexpectedly) lots of distinct car names in the dataset. The cars all seem to have 3, 4, 5, 6, or 8 cylinders (with 4 cylinder cars being the most common), and there's a relatively even number of examples from each year between 1970 and 1982 (though there are more from 1973). The cars are manufactured in Asia, Europe, or North America (with many more North American cars than Asian or European).\r\n",
    "\r\n",
    "Let's compare each of these categorical feature values with the label, by comparing the distribution and median *mpg* value for each category within each feature."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# plot a boxplot for the label by each categorical feature\r\n",
    "for col in categorical_features:\r\n",
    "    fig = plt.figure(figsize=(9, 6))\r\n",
    "    ax = fig.gca()\r\n",
    "    auto_data.boxplot(column = label.name, by = col, ax = ax)\r\n",
    "    ax.set_title(label.name +' by ' + col)\r\n",
    "    ax.set_ylabel(label.name)\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "There are definitely some differences in the label distributions depending on specific categories for cylinders, model year, and origin. It's hard to see any meaningful differences for car name because there are so many, and if you think about it logically, there's no real reason that the name of a car would influence its fuel efficiency.\r\n",
    "\r\n",
    "So, let's exclude the car name feature, and convert the data type of the others to *categorical* so we know to treat them as categories and not integer numeric values."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "auto_data_copy = auto_data.drop(['car_name'], axis=1).copy()\r\n",
    "\r\n",
    "print(\"Data Types:\")\r\n",
    "print(auto_data_copy.dtypes)\r\n",
    "\r\n",
    "# Change columns 0 and 5 to categorical\r\n",
    "cat_columns = [0,5]\r\n",
    "auto_data_copy.iloc[:,cat_columns] = auto_data_copy.iloc[:,cat_columns].astype(\"category\")\r\n",
    "auto_data_copy.dtypes\r\n",
    "\r\n",
    "print(\"\\nConverted Data Types:\")\r\n",
    "print(auto_data_copy.dtypes)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Machine learning algorithms generally work with numeric feature values - even if they're categorical, so we need to encode the text-based origins as integer representations. To do this, we'll use an ordinal encoder, which generates an integer code for each distinct origin value."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\r\n",
    "\r\n",
    "origin_encoder = OrdinalEncoder(dtype=int).fit(auto_data_copy[['origin']])\r\n",
    "\r\n",
    "origin_codes = origin_encoder.transform(auto_data_copy[['origin']])\r\n",
    "auto_data_copy['origin_code'] = origin_codes\r\n",
    "auto_data_copy.head(20)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that we have some categorical features, let's add them to the numeric features and use them to train a model."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Separate features and labels\r\n",
    "features = ['cylinders','displacement','horsepower','weight','accelleration','model_year','origin_code']\r\n",
    "X, y = auto_data_copy[features].values, label.values\r\n",
    "\r\n",
    "# Split data 70%-30% into training set and test set\r\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)\r\n",
    "\r\n",
    "# Fit a gradient boosting model on the training set\r\n",
    "model = GradientBoostingRegressor(learning_rate=0.1, n_estimators=100).fit(X_train, y_train)\r\n",
    "\r\n",
    "# test the model\r\n",
    "predictions = model.predict(X_test)\r\n",
    "\r\n",
    "mse = mean_squared_error(y_test, predictions)\r\n",
    "print(\"MSE:\", mse)\r\n",
    "\r\n",
    "rmse = np.sqrt(mse)\r\n",
    "print(\"RMSE:\", rmse)\r\n",
    "\r\n",
    "r2 = r2_score(y_test, predictions)\r\n",
    "print(\"R2:\", r2)\r\n",
    "\r\n",
    "plt.scatter(y_test, predictions)\r\n",
    "plt.xlabel('Actual Labels')\r\n",
    "plt.ylabel('Predicted Labels')\r\n",
    "plt.title('Predictions')\r\n",
    "# overlay the regression line\r\n",
    "z = np.polyfit(y_test, predictions, 1)\r\n",
    "p = np.poly1d(z)\r\n",
    "plt.plot(y_test,p(y_test), color='magenta')\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Pretty good!\r\n",
    "\r\n",
    "We previously encoded the *origin* feature using an ordinal integer value. An alternative approach is to use (*one-hot* encoding, in which a column for each possible category value is added to the dataset, and a 1 or 0 is used to indicate whether a row belongs to each category.\r\n",
    "\r\n",
    "Let's take a look."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\r\n",
    "\r\n",
    "auto_data_copy = auto_data_copy.drop(['origin_code'], axis=1).copy()\r\n",
    "\r\n",
    "origin_oh_encoder = OneHotEncoder(dtype=int)\r\n",
    "\r\n",
    "origin_oh_codes = origin_oh_encoder.fit_transform(auto_data[['origin']])\r\n",
    "auto_data_copy[origin_oh_encoder.get_feature_names()] = origin_oh_codes.toarray()\r\n",
    "auto_data_copy.head(20)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, instead of a single *origin_code* feature with a value of 0, 1, or 2; there are three columns (one for each possible origin - Asia, Europe, or North America), and each autombobile has a **1** for the feature representing its origin, and **0** for the others.\r\n",
    "\r\n",
    "Let's retrain the model using this encoding scheme."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Separate features (original columns plus encoder results) and labels\r\n",
    "features = np.append(['cylinders','displacement','horsepower','weight','accelleration','model_year'],origin_oh_encoder.get_feature_names())\r\n",
    "X, y = auto_data_copy[features].values, label.values\r\n",
    "\r\n",
    "# Split data 70%-30% into training set and test set\r\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)\r\n",
    "\r\n",
    "# Fit a linear regression model on the training set\r\n",
    "model = GradientBoostingRegressor(learning_rate=0.1, n_estimators=100).fit(X_train, y_train)\r\n",
    "\r\n",
    "# test the model\r\n",
    "predictions = model.predict(X_test)\r\n",
    "\r\n",
    "mse = mean_squared_error(y_test, predictions)\r\n",
    "print(\"MSE:\", mse)\r\n",
    "\r\n",
    "rmse = np.sqrt(mse)\r\n",
    "print(\"RMSE:\", rmse)\r\n",
    "\r\n",
    "r2 = r2_score(y_test, predictions)\r\n",
    "print(\"R2:\", r2)\r\n",
    "\r\n",
    "plt.scatter(y_test, predictions)\r\n",
    "plt.xlabel('Actual Labels')\r\n",
    "plt.ylabel('Predicted Labels')\r\n",
    "plt.title('Predictions')\r\n",
    "# overlay the regression line\r\n",
    "z = np.polyfit(y_test, predictions, 1)\r\n",
    "p = np.poly1d(z)\r\n",
    "plt.plot(y_test,p(y_test), color='magenta')\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Pre-process numeric features\r\n",
    "\r\n",
    "Now we're using a combination of numeric and categorical features, and getting pretty good results from the algorithm we've selected for model training. but there's one more optimziation we should explore. Currently, we're using the numeric values as-is, even though they're all measured on different scales. For example, *displacement* and *horsepower* are typically measured in the hundreds, while *weight* values are in the thousands, and *accelleration* is in the tens.\r\n",
    "\r\n",
    "Why is this a problem? Well, in some algorithms, larger values may be disporportionately affected by coefficients determined during training. Put simply, features with larger values may outweigh other, smaller features - even if the smaller features are more predictive.\r\n",
    "\r\n",
    "So what's the solution? We typically *normalize* the numeroic values so they're on a similar scale (for example, by assigning all numeric features a value between 0 and 1, relative to their actual unscaled values).\r\n",
    "\r\n",
    "Let's try that, using a MinMax scaler that assigns 0 the minimum and 1 to the maximum value for each feature, and assigns relative values between 0 and 1 to the others."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\r\n",
    "\r\n",
    "# Get a scaler object\r\n",
    "scaler = MinMaxScaler()\r\n",
    "\r\n",
    "# Normalize the numeric columns\r\n",
    "new_cols = ['displacement_norm','horsepower_norm','weight_norm','accelleration_norm']\r\n",
    "auto_data_copy[new_cols] = scaler.fit_transform(auto_data_copy[numeric_features])\r\n",
    "\r\n",
    "# Plot the normalized values\r\n",
    "auto_data_copy.head(20)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's train the model again, this time using the normalized numeric features."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "features = np.append(['cylinders','displacement_norm','horsepower_norm','weight_norm','accelleration_norm','model_year'],origin_oh_encoder.get_feature_names())\r\n",
    "X, y = auto_data_copy[features].values, label.values\r\n",
    "\r\n",
    "# Split data 70%-30% into training set and test set\r\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)\r\n",
    "\r\n",
    "# Fit a linear regression model on the training set\r\n",
    "model = GradientBoostingRegressor(learning_rate=0.1, n_estimators=100).fit(X_train, y_train)\r\n",
    "\r\n",
    "# test the model\r\n",
    "predictions = model.predict(X_test)\r\n",
    "\r\n",
    "mse = mean_squared_error(y_test, predictions)\r\n",
    "print(\"MSE:\", mse)\r\n",
    "\r\n",
    "rmse = np.sqrt(mse)\r\n",
    "print(\"RMSE:\", rmse)\r\n",
    "\r\n",
    "r2 = r2_score(y_test, predictions)\r\n",
    "print(\"R2:\", r2)\r\n",
    "\r\n",
    "plt.scatter(y_test, predictions)\r\n",
    "plt.xlabel('Actual Labels')\r\n",
    "plt.ylabel('Predicted Labels')\r\n",
    "plt.title('Predictions')\r\n",
    "# overlay the regression line\r\n",
    "z = np.polyfit(y_test, predictions, 1)\r\n",
    "p = np.poly1d(z)\r\n",
    "plt.plot(y_test,p(y_test), color='magenta')\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create a pipeline\r\n",
    "\r\n",
    "Alright, now we have a model that seems to produce reasonably accurate predictions. However it's based on features that we had to perform some pre-processing tasks on to prepare them for the model, so when used to predict the *mpg* for a new car, it will only work if we perform the same preprocessing steps to the new features.\r\n",
    "\r\n",
    "A better approach is to encapsulate all of the pre-processing as well as the model itself into a *pipeline*. We can use a pipeline to prepare the data and train the model, similarly to before, but the model that it produces includes all of the pre-processing steps - so we can submit new car data as it is, and the model will do all the necessary encoding and scaling before generating a prediction.\r\n",
    "\r\n",
    "To see how this work, let's use a pipeline to nromalize the numeric features, encode the categorical features, and train a model."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Train the model\r\n",
    "from sklearn.compose import ColumnTransformer\r\n",
    "from sklearn.pipeline import Pipeline\r\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\r\n",
    "from sklearn.ensemble import GradientBoostingRegressor\r\n",
    "\r\n",
    "# Separate features and labels\r\n",
    "features = ['cylinders','displacement','horsepower','weight','accelleration','model_year','origin']\r\n",
    "X, y = auto_data[features].values, label.values\r\n",
    "\r\n",
    "# Split data 70%-30% into training set and test set\r\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)\r\n",
    "\r\n",
    "# Define preprocessing for numeric columns (scale them)\r\n",
    "numeric_features = [1,2,3,4]\r\n",
    "numeric_transformer = Pipeline(steps=[\r\n",
    "    ('scaler', MinMaxScaler())])\r\n",
    "\r\n",
    "# Define preprocessing for categorical features (one-hot encode them)\r\n",
    "categorical_features = [0,5,6]\r\n",
    "categorical_transformer = Pipeline(steps=[\r\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\r\n",
    "\r\n",
    "# Combine preprocessing steps\r\n",
    "preprocessor = ColumnTransformer(\r\n",
    "    transformers=[\r\n",
    "        ('num', numeric_transformer, numeric_features),\r\n",
    "        ('cat', categorical_transformer, categorical_features)],\r\n",
    "    remainder = 'drop' # Remove any other features\r\n",
    "    )\r\n",
    "\r\n",
    "# Create preprocessing and training pipeline\r\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\r\n",
    "                           ('regressor', GradientBoostingRegressor(learning_rate=0.1, n_estimators=100))])\r\n",
    "\r\n",
    "\r\n",
    "# fit the pipeline to train a linear regression model on the training set\r\n",
    "model = pipeline.fit(X_train, y_train)\r\n",
    "print (model)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The output shows the pipeline, which includes steps for normalizing and encoding before applying the trained Gradient Boosting regressor.\r\n",
    "\r\n",
    "let's try it with our test data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "predictions = model.predict(X_test)\r\n",
    "\r\n",
    "mse = mean_squared_error(y_test, predictions)\r\n",
    "print(\"MSE:\", mse)\r\n",
    "\r\n",
    "rmse = np.sqrt(mse)\r\n",
    "print(\"RMSE:\", rmse)\r\n",
    "\r\n",
    "r2 = r2_score(y_test, predictions)\r\n",
    "print(\"R2:\", r2)\r\n",
    "\r\n",
    "plt.scatter(y_test, predictions)\r\n",
    "plt.xlabel('Actual Labels')\r\n",
    "plt.ylabel('Predicted Labels')\r\n",
    "plt.title('Predictions')\r\n",
    "# overlay the regression line\r\n",
    "z = np.polyfit(y_test, predictions, 1)\r\n",
    "p = np.poly1d(z)\r\n",
    "plt.plot(y_test,p(y_test), color='magenta')\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you find that encapsulating your training process in a pipline results in a drop in performance, you can easily experiment with alternative algorithms just by changing the algorithm step, and using the same pre-processing transformer steps as before."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.linear_model import LinearRegression\r\n",
    "\r\n",
    "# Create preprocessing and training pipeline\r\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\r\n",
    "                           ('regressor', LinearRegression())])\r\n",
    "\r\n",
    "\r\n",
    "# fit the pipeline to train a linear regression model on the training set\r\n",
    "model = pipeline.fit(X_train, (y_train))\r\n",
    "print (model)\r\n",
    "\r\n",
    "\r\n",
    "predictions = model.predict(X_test)\r\n",
    "\r\n",
    "mse = mean_squared_error(y_test, predictions)\r\n",
    "print(\"MSE:\", mse)\r\n",
    "\r\n",
    "rmse = np.sqrt(mse)\r\n",
    "print(\"RMSE:\", rmse)\r\n",
    "\r\n",
    "r2 = r2_score(y_test, predictions)\r\n",
    "print(\"R2:\", r2)\r\n",
    "\r\n",
    "plt.scatter(y_test, predictions)\r\n",
    "plt.xlabel('Actual Labels')\r\n",
    "plt.ylabel('Predicted Labels')\r\n",
    "plt.title('Predictions')\r\n",
    "# overlay the regression line\r\n",
    "z = np.polyfit(y_test, predictions, 1)\r\n",
    "p = np.poly1d(z)\r\n",
    "plt.plot(y_test,p(y_test), color='magenta')\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Save and use the model\r\n",
    "\r\n",
    "Now that we have a working model, let's save it so we can use it again later.\r\n",
    "\r\n",
    "To save the model, we'll use the *pickle* library (to preserve it - geddit?)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import joblib\r\n",
    "\r\n",
    "# Save the model as a pickle file\r\n",
    "filename = './models/auto.pkl'\r\n",
    "joblib.dump(model, filename)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "With the model saved, we can load it again later when we need it to predict the *mpg* for a new car."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Load the model from the file\r\n",
    "loaded_model = joblib.load(filename)\r\n",
    "\r\n",
    "# Create a numpy array containing details of a new car\r\n",
    "X_new = np.array([[8,302,140,3449,10.5,1970,'North America']])\r\n",
    "print ('New sample: {}'.format(list(X_new[0])))\r\n",
    "\r\n",
    "# Use the model to predict mpg\r\n",
    "result = loaded_model.predict(X_new)\r\n",
    "print('Prediction: {:.0f} mpg'.format(result[0]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The model's **predict** method accepts an array of observations, so you can use it to generate multiple predictions as a batch. For example, suppose you have details for three new cars; you could use the model to predict *mpg* for each car."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# An array of features based on three new cars\r\n",
    "X_new = np.array([[8,302,140,3449,10.5,1970,'North America'],\r\n",
    "                  [4,97,46,1835,20.5,1970,'Europe'],\r\n",
    "                  [4,97,88,2130,14.5,1971,'Asia'],])\r\n",
    "\r\n",
    "# Use the model to predict mpg\r\n",
    "results = loaded_model.predict(X_new)\r\n",
    "print('MPG predictions:')\r\n",
    "for prediction in results:\r\n",
    "    print(np.round(prediction))"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "d631c4f8937aff5a00d938bb761b79b71fe773d07f3a79c191fcf956b8a61c95"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}